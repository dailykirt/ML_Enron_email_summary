{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing Emails using Machine Learning: Data Wrangling\n",
    "## Table of Contents\n",
    "1. Imports & Initalization <br>\n",
    "2. Retrieve Preprocessed Data <br>\n",
    "3. TextRank Modeling <br>\n",
    "    A. Prepare Model <br>\n",
    "    B. BC3 Corpus Summary <br>\n",
    "    C. BC3 Corpus ROUGE Evaluation <br>\n",
    "    D. Enron Dataset Summary <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the TextRank algorithim found at:\n",
    "https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
    "\n",
    "The algorithim will generate summaries of someone's inbox over a period of time. The BC3 Corpus contains human summaries that can be used to generate ROUGE metrics to better understand the accuracy of the summarizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rouge\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Preprocessed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pickled dataframe produced by the Process_Emails notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "ENRON_PICKLE_LOC = \"../data/dataframes/wrangled_enron_full_df.pkl\"\n",
    "BC3_EMAIL_PICKLE_LOC = \"../data/dataframes/wrangled_BC3_email_df.pkl\"\n",
    "BC3_SUMMARY_PICKLE_LOC = \"../data/dataframes/wrangled_BC3_summary_df.pkl\"\n",
    "\n",
    "enron_df = pd.read_pickle(ENRON_PICKLE_LOC)\n",
    "BC3_emails_df = pd.read_pickle(BC3_EMAIL_PICKLE_LOC)\n",
    "BC3_summary_df = pd.read_pickle(BC3_SUMMARY_PICKLE_LOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ML algorithim uses the GloVe word embeddings generated by the pre-trained Wikipedia 2014 + Gigaword 5 mode. Below are one time executions to download this model locally.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#! unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_vectors():\n",
    "    #This returns word vectors from the pretrained glove model. \n",
    "    word_embeddings = {}\n",
    "    f = open('glove.6B.300d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs\n",
    "    f.close()\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract word vectors. Only need to be done once. \n",
    "word_embeddings = extract_word_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve original email sentences and index them. This will be used to generate the extracted summaries. \n",
    "def get_extractive_sentences(df):\n",
    "    sentences = df.Extractive_Sentences.tolist()\n",
    "    #flatten list as tuples containting (sentence, dataframe index)  to reassociate summary with original email. \n",
    "    sentences = []\n",
    "    sentences_list = df.Extractive_Sentences.tolist()\n",
    "    for counter, sublist in enumerate(sentences_list):\n",
    "        for item in sublist:\n",
    "            sentences.append([counter, item]) \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The tokenized sentences were done during preprocessing, \n",
    "#so this function retrieves them from the dataframe, then flattens the list. \n",
    "def get_tokenized_sentences(df):\n",
    "    clean_sentences = df.Tokenized_Body.tolist()\n",
    "    #flatten list\n",
    "    clean_sentences = [y for x in clean_sentences for y in x]\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sentence_vectors for each tokenized sentence using the word_embeddings model. \n",
    "def create_sentence_vectors(clean_sentences, word_embeddings):\n",
    "    sentence_vectors = []\n",
    "    for i in clean_sentences:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((300,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((300,))\n",
    "        sentence_vectors.append(v)\n",
    "    return sentence_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences(sentences, sentence_vectors):\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "    #Initialize matrix with cosine similarity scores. \n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "              sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,300), sentence_vectors[j].reshape(1,300))[0,0]\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    #Pair sentence with it's similarity score then sort. \n",
    "    ranked_sentences = sorted(((scores[i],s[0],s[1]) for i,s in enumerate(sentences)), reverse=True)\n",
    "    #ranked_sentences = ((scores[i],s[0],s[1]) for i,s in enumerate(sentences))\n",
    "    return ranked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to wrap up summarization process\n",
    "def summarize_emails(word_embeddings, masked_df):\n",
    "    print(\"Total number of emails to summarize: \" + str(len(masked_df)))\n",
    "    sentences = get_extractive_sentences(masked_df)\n",
    "    clean_sentences = get_tokenized_sentences(masked_df)\n",
    "    #Generate sentence vectors\n",
    "    sentence_vectors = create_sentence_vectors(clean_sentences, word_embeddings)\n",
    "    #Create a list of ranked sentences. \n",
    "    ranked_sentences = rank_sentences(sentences, sentence_vectors)\n",
    "    #return enron_masked_df, ranked_sentences\n",
    "    #display_summary(masked_df, ranked_sentences)\n",
    "    return ranked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#color scheme to help distinguish summarizaiton text. \n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "def display_summary(enron_masked_df, ranked_sentences):\n",
    "  # Specify number of sentences as a fraction of total emails. \n",
    "  sn = len(enron_masked_df) // 10\n",
    "\n",
    "  # Generate summary\n",
    "  for i in range(sn):\n",
    "    #pull date and subject from original email\n",
    "    email_date = str(enron_masked_df['Date'].iloc[ranked_sentences[i][1]])\n",
    "    email_subject = str(enron_masked_df['Subject'].iloc[ranked_sentences[i][1]])\n",
    "    email_from = str(enron_masked_df['From'].iloc[ranked_sentences[i][1]])\n",
    "    print( bcolors.BOLD + \"Date: \"+ email_date  + \n",
    "          \" Subject: \" + email_subject +\n",
    "          \" From: \" + email_from + bcolors.ENDC +\n",
    "          \"\\nSummary: \" + str(ranked_sentences[i][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BC3 Corpus Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of emails to summarize: 1\n"
     ]
    }
   ],
   "source": [
    "#First look into what a single email summary looks like. \n",
    "masked_df = BC3_emails_df[:1]\n",
    "masked_summaries = BC3_summary_df['Summary'].loc[(BC3_summary_df['Listno'] == masked_df.iloc[0]['Listno']) & (BC3_summary_df['Email_num'] == str(masked_df['Email_num'].iloc[0]))]\n",
    "ranked_sentences = summarize_emails(word_embeddings, masked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(masked_df, ranked_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object rank_sentences.<locals>.<genexpr> at 0x7fb753638ed0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BC3 Corpus ROUGE Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rouge metric (https://pypi.org/project/py-rouge/) is an evaluation metric used to test machine generated summaries against a human \"Gold standard\". Using the same Text rank summarization methods used on the Enron dataset, the following evaluates the algorithim against the BC3 Corpus. This is one of the few email datasets that contain human summarizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are three different human summaries for the same email. \n",
    "masked_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = rouge.Rouge(metrics=['rouge-n'],\n",
    "                           max_n=1,\n",
    "                           limit_length=True,\n",
    "                           length_limit=100,\n",
    "                           length_limit_type='words',\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)\n",
    "full_body = masked_df['Body'].iloc[0]\n",
    "hypothesis = ranked_sentences[0][2]\n",
    "reference = masked_summaries.iloc[0]\n",
    "\n",
    "#scores = evaluator.get_scores(hypothesis, reference)\n",
    "print(\"Full Email: \" + full_body + '\\n')\n",
    "print(\"ML Summary: \" + hypothesis + \"\\n\") \n",
    "print(\"Human Summary: \" + reference + \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the R-1 scores. Current benchmarks for text summarization can be found at: https://summari.es/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_score = evaluator.get_scores(reference, reference)\n",
    "scores = evaluator.get_scores(hypothesis, reference)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enron Dataset Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outputs a subset of the enron dataset masked by the person and a timeframe. \n",
    "def subset_emails(df, start_date, end_date, person):   \n",
    "    summarization_mask = (enron_df['Date'] >= start_date) & (enron_df['Date'] <= end_date) & (enron_df['Employee'] == person)\n",
    "    enron_masked_df = df.loc[summarization_mask]\n",
    "    return enron_masked_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of emails to summarize: 72\n"
     ]
    }
   ],
   "source": [
    "#Define emails to be summarized. \n",
    "start_date = '2001-10-01 00:00:00'\n",
    "end_date = '2001-10-14 23:59:59'\n",
    "person = 'skilling-j'\n",
    "masked_df = subset_emails(enron_df, start_date, end_date, person)\n",
    "ranked_sentences = summarize_emails(word_embeddings, masked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDate: 2001-10-02 21:51:28 Subject: Test From: dalak@hotmail.com\u001b[0m\n",
      "Summary: Please ignore.\n",
      "\u001b[1mDate: 2001-10-02 21:51:28 Subject: Test From: dalak@hotmail.com\u001b[0m\n",
      "Summary: Get your FREE download of MSN Explorer at http://explorer.msn.com/intl.asp\n",
      "\u001b[1mDate: 2001-10-01 19:45:15 Subject: The Morning Market Call - Monday October 1st, 2001. From: david.morris@lehman.com\u001b[0m\n",
      "Summary: Good Monday Morning Comments\n",
      "\u001b[1mDate: 2001-10-04 22:45:18 Subject: Solar Migration - Third Notice - Time Change!!!!! From: bob.ambrocik@enron.com\u001b[0m\n",
      "Summary: During the weekend of October 6 and 7, 2001 the Enterprise Storage Team will be migrating all production users off the current hardware (Solar) that houses their home and application directories (no production databases are affected, but client software will be) to new hardware.\n",
      "\u001b[1mDate: 2001-10-04 22:45:18 Subject: Solar Migration - Third Notice - Time Change!!!!! From: bob.ambrocik@enron.com\u001b[0m\n",
      "Summary: This migration requires a total system outage of approximately 6 hours.\n",
      "\u001b[1mDate: 2001-10-04 22:45:18 Subject: Solar Migration - Third Notice - Time Change!!!!! From: bob.ambrocik@enron.com\u001b[0m\n",
      "Summary: The outage will occur Saturday night beginning at 7:00 PM and will last until Sunday morning at 1:00 AM.\n",
      "\u001b[1mDate: 2001-10-04 22:45:18 Subject: Solar Migration - Third Notice - Time Change!!!!! From: bob.ambrocik@enron.com\u001b[0m\n",
      "Summary: All users will need to be logged off during this time period.\n"
     ]
    }
   ],
   "source": [
    "display_summary(masked_df, list(ranked_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examples of a full email. \n",
    "masked_df['Body'].iloc[ranked_list[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarization from another inbox\n",
    "start_date = '2001-10-01 00:00:00'\n",
    "end_date = '2001-10-14 23:59:59'\n",
    "person = 'arnold-j'\n",
    "masked_df = subset_emails(enron_df, start_date, end_date, person)\n",
    "ranked_sentences = summarize_emails(word_embeddings, masked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examples of a full email. \n",
    "masked_df['Body'].iloc[ranked_sentences[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One more example\n",
    "start_date = '2001-10-01 00:00:00'\n",
    "end_date = '2001-10-14 23:59:59'\n",
    "person = 'lenhart-m'\n",
    "masked_df = subset_emails(enron_df, start_date, end_date, person)\n",
    "ranked_sentences = summarize_emails(word_embeddings, masked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examples of a full email. \n",
    "masked_df['Body'].iloc[ranked_sentences[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
