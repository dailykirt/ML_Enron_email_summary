{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing Emails using Machine Learning\n",
    "## Table of Contents\n",
    "1. Imports & Initalization <br>\n",
    "2. Data Input <br>\n",
    "    A. Enron Email Dataset <br>\n",
    "    B. BC3 Corpus <br>\n",
    "3. Preprocessing <br>\n",
    "    A. Delete bad data. <br>\n",
    "    B. Sentence Cleaning <br>\n",
    "    C. Tokenizing <br>\n",
    "4. Store Data\n",
    "    A. Locally as pickle\n",
    "    B. Into database. \n",
    "5. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to clean both the Enron Email and BC3 Corpus data sets to perform email text summarization. The BC3 Corpus contains human summarizations that can be used to calculate ROUGE metrics to better understand how accurate the summarizations are. The Enron dataset is far more comprehensive, but lacks summaries to test against. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirt/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#File system / database libraries\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import configparser\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#Data science tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Email cleaning \n",
    "import email\n",
    "import mailparser\n",
    "import xml.etree.ElementTree as ET\n",
    "from talon.signature.bruteforce import extract_signature\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "#Parallelizaiton \n",
    "import dask.dataframe as dd\n",
    "from distributed import Client\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set local location of emails. \n",
    "mail_dir = '../data/maildir/'\n",
    "#mail_dir = '../data/testdir/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input: \n",
    "### A. Enron Email Dataset\n",
    "The raw enron email dataset contains a maildir directory that contains folders seperated by employee which contain the emails. The following processes the raw text of each email into a dask dataframe with the following columns: \n",
    "\n",
    "Employee: The username of the email owner. <br>\n",
    "Body: Cleaned body of the email. <br>\n",
    "Subject: The title of the email. <br>\n",
    "From: The original sender of the email <br>\n",
    "Message-ID: Used to remove duplicate emails, as each email has a unique ID. <br>\n",
    "Chain: The parsed out email chain from a email that was forwarded. <br>\n",
    "Signature: The extracted signature from the body.<br>\n",
    "Date: Time the email was sent. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_email(index):\n",
    "    #This function attempts to split a raw email into constituent parts that can be used as features. \n",
    "    email_path = index[0]\n",
    "    employee = index[1]\n",
    "    folder = index[2]\n",
    "    \n",
    "    mail = mailparser.parse_from_file(email_path)\n",
    "    full_body = email.message_from_string(mail.body)\n",
    "    \n",
    "    #Only retrieve the body of the email. \n",
    "    if full_body.is_multipart():\n",
    "        return\n",
    "    else:\n",
    "        mail_body = full_body.get_payload()    \n",
    "    \n",
    "    split_body = clean_body(mail_body)\n",
    "    headers = mail.headers\n",
    "    #Reformating date to be more pandas readable\n",
    "    date_time = process_date(headers.get('Date'))\n",
    "\n",
    "    email_dict = {\n",
    "                \"employee\" : employee,\n",
    "                \"email_folder\": folder,\n",
    "                \"message_id\": headers.get('Message-ID'),\n",
    "                \"date\" : date_time,\n",
    "                \"from\" : headers.get('From'),\n",
    "                \"subject\": headers.get('Subject'),\n",
    "                \"body\" : split_body['body'],\n",
    "                \"chain\" : split_body['chain'],\n",
    "                \"signature\": split_body['signature'],\n",
    "                \"full_email_path\" : email_path #for debug purposes. \n",
    "    }\n",
    "    \n",
    "    #Append row to dataframe. \n",
    "    return email_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_body(mail_body):\n",
    "    delimiters = [\"-----Original Message-----\",\"To:\",\"From\"]\n",
    "    \n",
    "    #Trying to split string by biggest delimiter. \n",
    "    old_len = sys.maxsize\n",
    "    \n",
    "    for delimiter in delimiters:\n",
    "        split_body = mail_body.split(delimiter,1)\n",
    "        new_len = len(split_body[0])\n",
    "        if new_len <= old_len:\n",
    "            old_len = new_len\n",
    "            final_split = split_body\n",
    "            \n",
    "    #Then pull chain message\n",
    "    if (len(final_split) == 1):\n",
    "        mail_chain = None\n",
    "    else:\n",
    "        mail_chain = final_split[1] \n",
    "    \n",
    "    #The following uses Talon to try to get a clean body, and seperate out the rest of the email. \n",
    "    clean_body, sig = extract_signature(final_split[0])\n",
    "    \n",
    "    return {'body': clean_body, 'chain' : mail_chain, 'signature': sig}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date(date_time):\n",
    "    try:\n",
    "        date_time = email.utils.format_datetime(email.utils.parsedate_to_datetime(date_time))\n",
    "    except:\n",
    "        date_time = None\n",
    "    return date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_email_paths(mail_dir):\n",
    "    #Generator to list each email path\n",
    "    mailboxes = listdir(mail_dir)\n",
    "    for mailbox in mailboxes:\n",
    "        inbox = listdir(mail_dir + mailbox)\n",
    "        for folder in inbox:\n",
    "            path = mail_dir + mailbox + \"/\" + folder\n",
    "            emails = listdir(path)\n",
    "            for single_email in emails:\n",
    "                full_path = path + \"/\" + single_email\n",
    "                if isfile(full_path): #Skip directories.\n",
    "                    yield (full_path, mailbox, folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use multiprocessing to speed up initial data load and processing. Also helps partition DASK dataframe. \n",
    "try:\n",
    "    cpus = mp.cpu_count()\n",
    "except NotImplementedError:\n",
    "    cpus = 2\n",
    "pool = mp.Pool(processes=cpus)\n",
    "print(\"CPUS: \" + str(cpus))\n",
    "\n",
    "indexes = generate_email_paths(mail_dir)\n",
    "enron_email_df = pool.map(process_email,indexes)\n",
    "enron_email_df = pd.DataFrame(enron_email_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_email_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input: \n",
    "### B. BC3 Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is split into two xml files. One contains the original emails split line by line, and the other contains the summarizations created by the annotators. Each email may contain several summarizations from different annotators and summarizations may also be over several emails. I will create a data frame for both xml files, then join them together using the thread number in combination of the email number for a single final dataframe. \n",
    "\n",
    "The first dataframe will contain the wrangled original emails containing the following information:\n",
    "\n",
    "Listno: Thread identifier <br>\n",
    "Email_num: Email in thread sequence <br>\n",
    "From: The original sender of the email <br>\n",
    "To: The recipient of the email. <br>\n",
    "Recieved: Time email was recieved. <br>\n",
    "Subject: Title of email. <br>\n",
    "Body: Original body. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bc3_emails(root):\n",
    "    BC3_email_list = []\n",
    "    #The emails are seperated by threads.\n",
    "    for thread in root:\n",
    "        email_num = 0\n",
    "        #Iterate through the thread elements <name, listno, Doc>\n",
    "        for thread_element in thread:\n",
    "            #Getting the listno allows us to link the summaries to the correct emails\n",
    "            if thread_element.tag == \"listno\":\n",
    "                listno = thread_element.text\n",
    "            #Each Doc element is a single email\n",
    "            if thread_element.tag == \"DOC\":\n",
    "                email_num += 1\n",
    "                email_metadata = []\n",
    "                for email_attribute in thread_element:\n",
    "                    #If the email_attri is text, then each child contains a line from the body of the email\n",
    "                    if email_attribute.tag == \"Text\":\n",
    "                        email_body = \"\"\n",
    "                        for sentence in email_attribute:\n",
    "                            email_body += sentence.text\n",
    "                    else:\n",
    "                        #The attributes of the Email <Recieved, From, To, Subject, Text> appends in this order. \n",
    "                        email_metadata.append(email_attribute.text)\n",
    "                        \n",
    "                #Use same enron cleaning methods on the body of the email\n",
    "                split_body = clean_body(email_body)\n",
    "                    \n",
    "                email_dict = {\n",
    "                    \"listno\" : listno,\n",
    "                    \"date\" : process_date(email_metadata[0]),\n",
    "                    \"from\" : email_metadata[1],\n",
    "                    \"to\" : email_metadata[2],\n",
    "                    \"subject\" : email_metadata[3],\n",
    "                    \"body\" : split_body['body'],\n",
    "                    \"email_num\": email_num\n",
    "                }\n",
    "                \n",
    "                BC3_email_list.append(email_dict)           \n",
    "    return pd.DataFrame(BC3_email_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load BC3 Email Corpus. Much smaller dataset has no need for parallel processing. \n",
    "parsedXML = ET.parse( \"../data/BC3_Email_Corpus/corpus.xml\" )\n",
    "root = parsedXML.getroot()\n",
    "\n",
    "#Clean up BC3 emails the same way as the Enron emails. \n",
    "bc3_email_df = parse_bc3_emails(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc3_email_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc3_email_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second dataframe contains the summarizations of each email:\n",
    "\n",
    "Annotator: Person who created summarization. <br>\n",
    "Email_num: Email in thread sequence. <br>\n",
    "Listno: Thread identifier. <br>\n",
    "Summary: Human summarization of the email. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bc3_summaries(root):\n",
    "    BC3_summary_list = []\n",
    "    for thread in root:\n",
    "        #Iterate through the thread elements <listno, name, annotation>\n",
    "        for thread_element in thread:\n",
    "            if thread_element.tag == \"listno\":\n",
    "                listno = thread_element.text\n",
    "            #Each Doc element is a single email\n",
    "            if thread_element.tag == \"annotation\":\n",
    "                for annotation in thread_element:\n",
    "                #If the email_attri is summary, then each child contains a summarization line\n",
    "                    if annotation.tag == \"summary\":\n",
    "                        summary_dict = {}\n",
    "                        for summary in annotation:\n",
    "                            #Generate the set of emails the summary sentence belongs to (often a single email)\n",
    "                            email_nums = summary.attrib['link'].split(',')\n",
    "                            s = set()\n",
    "                            for num in email_nums:\n",
    "                                s.add(num.split('.')[0].strip()) \n",
    "                            #Remove empty strings, since they summarize whole threads instead of emails. \n",
    "                            s = [x for x in set(s) if x]\n",
    "                            for email_num in s:\n",
    "                                if email_num in summary_dict:\n",
    "                                    summary_dict[email_num] += ' ' + summary.text\n",
    "                                else:\n",
    "                                    summary_dict[email_num] = summary.text\n",
    "                    #get annotator description\n",
    "                    elif annotation.tag == \"desc\":\n",
    "                        annotator = annotation.text\n",
    "                #For each email summarizaiton create an entry\n",
    "                for email_num, summary in summary_dict.items():\n",
    "                    email_dict = {\n",
    "                        \"listno\" : listno,\n",
    "                        \"annotator\" : annotator,\n",
    "                        \"email_num\" : email_num,\n",
    "                        \"summary\" : summary\n",
    "                    }      \n",
    "                    BC3_summary_list.append(email_dict)\n",
    "    return pd.DataFrame(BC3_summary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load summaries and process\n",
    "parsedXML = ET.parse( \"../data/BC3_Email_Corpus/annotation.xml\" )\n",
    "root = parsedXML.getroot()\n",
    "bc3_summary_df = parse_bc3_summaries(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc3_summary_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Initalization: \n",
    "### A. Cleaning bad data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert date to pandas datetime.\n",
    "enron_email_df['date'] = pd.to_datetime(enron_email_df['date'], utc=True)\n",
    "bc3_email_df['date'] = pd.to_datetime(bc3_email_df.date, utc=True)\n",
    "\n",
    "#Look at the timeframe\n",
    "start_date = str(enron_email_df.date.min())\n",
    "end_date =  str(enron_email_df.date.max())\n",
    "print(\"Start Date: \" + start_date)\n",
    "print(\"End Date: \" + end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the data was collected in May 2002 according to wikipedia, its a bit strange to see emails past that date. \n",
    "#Reading some of the emails seem to suggest it's mostly spam. \n",
    "enron_email_df[(enron_email_df.date > '2003-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick look at emails before 1999, \n",
    "enron_email_df[(enron_email_df.date < '1999-01-01')].date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_email_df[(enron_email_df.date == '1980-01-01')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The emails seem legetimate, but there seems to be a glut of emails dated exactly on 1980-01-01. \n",
    "#Keep emails between Jan 1st 1999 and June 1st 2002. \n",
    "enron_email_df = enron_email_df[(enron_email_df.date > '1998-01-01') & (enron_email_df.date < '2002-06-01')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Sentence Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw enron email Corpus tends to have a large amount of unneeded characters that can interfere with tokenizaiton. It's best to do a bit more cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_email_df(df):\n",
    "    #Removing strings related to attatchments and certain non numerical characters.\n",
    "    patterns = [\"\\[IMAGE\\]\",\"-\", \"_\", \"\\*\", \"+\",\"\\\".\\\"\"]\n",
    "    for pattern in patterns:\n",
    "        df['body'] = pd.Series(df['body']).str.replace(pattern, \"\")\n",
    "    \n",
    "    #Remove multiple spaces. \n",
    "    df['body'] = df['body'].replace('\\s+', ' ', regex=True)\n",
    "\n",
    "    #Blanks are replaced with NaN in the whole dataframe. Then rows with a 'NaN' in the body will be dropped. \n",
    "    df = df.replace('',np.NaN)\n",
    "    df = df.dropna(subset=['body'])\n",
    "\n",
    "    #Remove all Duplicate emails \n",
    "    df = df.drop_duplicates(subset='body')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply clean to both datasets. \n",
    "enron_email_df = clean_email_df(enron_email_df)\n",
    "bc3_email_df = clean_email_df(bc3_email_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to split up sentences into it's constituent parts for the ML algorithim that will be used for text summarization. This will be applied to both the Enron and BC3 datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sen):\n",
    "    #This function removes stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new\n",
    "\n",
    "def tokenize_email(text):\n",
    "    #This function splits up the body into sentence tokens and removes stop words. \n",
    "    clean_sentences = sent_tokenize(text, language='english')\n",
    "    #removing punctuation, numbers and special characters. Then lowercasing. \n",
    "    clean_sentences = [re.sub('[^a-zA-Z ]', '',s) for s in clean_sentences]\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the Enron dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This tokenizing will be the extracted sentences that may be chosen to form the email summaries. \n",
    "enron_email_df['extractive_sentences'] = enron_email_df['body'].apply(sent_tokenize)\n",
    "#Splitting the text in emails into cleaned sentences\n",
    "enron_email_df['tokenized_body'] = enron_email_df['body'].apply(tokenize_email)\n",
    "#Tokenizing the bodies might have revealed more duplicate emails that should be droped. \n",
    "enron_email_df = enron_email_df.loc[enron_email_df.astype(str).drop_duplicates(subset='tokenized_body').index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now working on the BC3 Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc3_email_df['extractive_sentences'] = bc3_email_df['body'].apply(sent_tokenize)\n",
    "bc3_email_df['tokenized_body'] = bc3_email_df['body'].apply(tokenize_email)\n",
    "bc3_email_df = bc3_email_df.loc[bc3_email_df.astype(str).drop_duplicates(subset='tokenized_body').index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Data\n",
    "### Locally as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local locations for pickle files. \n",
    "ENRON_PICKLE_LOC = \"../data/dataframes/wrangled_enron_full_df.pkl\"\n",
    "BC3_EMAIL_PICKLE_LOC = \"../data/dataframes/wrangled_BC3_email_df.pkl\"\n",
    "BC3_SUMMARY_PICKLE_LOC = \"../data/dataframes/wrangled_BC3_summary_df.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store dataframes to disk\n",
    "#enron_email_df.to_pickle(ENRON_PICKLE_LOC)\n",
    "#bc3_email_df.to_pickle(BC3_EMAIL_PICKLE_LOC)\n",
    "#bc3_summary_df.to_pickle(BC3_SUMMARY_PICKLE_LOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store data into a Postgres Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure postgres database\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config_notebook.ini')\n",
    "\n",
    "#database_config = 'LOCAL_POSTGRES'\n",
    "database_config = 'AWS_POSTGRES'\n",
    "\n",
    "POSTGRES_ADDRESS = config[database_config]['POSTGRES_ADDRESS']\n",
    "POSTGRES_USERNAME = config[database_config]['POSTGRES_USERNAME']\n",
    "POSTGRES_PASSWORD = config[database_config]['POSTGRES_PASSWORD']\n",
    "POSTGRES_DBNAME = config[database_config]['POSTGRES_DBNAME']\n",
    "\n",
    "#now create database connection\n",
    "postgres_str = ('postgresql+psycopg2://{username}:{password}@{ipaddress}/{dbname}'\n",
    "                .format(username=POSTGRES_USERNAME, \n",
    "                        password=POSTGRES_PASSWORD,\n",
    "                        ipaddress=POSTGRES_ADDRESS,\n",
    "                        dbname=POSTGRES_DBNAME))\n",
    "\n",
    "cnx = create_engine(postgres_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store data. \n",
    "#enron_email_df.to_sql('full_enron_emails', cnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dask can help speed up exploration computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(processes = True)\n",
    "client.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make into dask dataframe. \n",
    "enron_email_df = dd.from_pandas(enron_email_df, npartitions=cpus)\n",
    "enron_email_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to create a describe summary of the dataset. Ignoring tokenized columns. \n",
    "enron_email_df[['body', 'chain', 'date', 'email_folder', 'employee', 'from', 'full_email_path', 'message_id', 'signature', 'subject']].describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get word frequencies from tokenized word lists\n",
    "def get_word_freq(df):\n",
    "    freq_words=dict()\n",
    "    for tokens in df.tokenized_words.compute():\n",
    "        for token in tokens:\n",
    "            if token in freq_words:\n",
    "                freq_words[token] += 1\n",
    "            else: \n",
    "                freq_words[token] = 1\n",
    "    return freq_words     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(sentences):\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the sentences \n",
    "enron_email_df['tokenized_words'] = enron_email_df['tokenized_body'].apply(tokenize_word).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating word dictionary to understand word frequencies. \n",
    "freq_words = get_word_freq(enron_email_df)\n",
    "print('Unique words: {:,}'.format(len(freq_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_data = []\n",
    "#Sort dictionary by highest word frequency. \n",
    "for key, value in sorted(freq_words.items(), key=lambda item: item[1], reverse=True):\n",
    "    word_data.append([key, freq_words[key]])\n",
    "\n",
    "#Prepare to plot bar graph of top words. \n",
    "#Create dataframe with Word and Frequency, then sort in Descending order. \n",
    "freq_words_df = pd.DataFrame.from_dict(freq_words, orient='index').reset_index()\n",
    "freq_words_df = freq_words_df.rename(columns={\"index\": \"Word\", 0: \"Frequency\"})\n",
    "freq_words_df = freq_words_df.sort_values(by=['Frequency'],ascending = False)\n",
    "freq_words_df.reset_index(drop = True, inplace=True)\n",
    "freq_words_df.head(30).plot(x='Word', kind='bar', figsize=(20,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
