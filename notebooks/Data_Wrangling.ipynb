{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to clean the Enron Email data set to perform email text summarization: Given a single person’s folder and a time range give a summary for each email. \n",
    "\n",
    "My approach to cleaning the data involves the following steps:\n",
    "\n",
    "1. I have unpacked the maildir directory from the Enron data set. I will be focusing on each person’s inbox, which means all other folders will be ignored for now.\n",
    "\n",
    "2. I will process each inbox email to add the following information to a Pandas data frame:\n",
    "\n",
    "Employee: The username of the email owner. <br>\n",
    "Body: Cleaned body of the email. <br>\n",
    "Subject: The title of the email. <br>\n",
    "From: The original sender of the email <br>\n",
    "Message-ID: Used to remove duplicate emails, as each email has a unique ID. <br>\n",
    "Chain: The parsed out email chain from a email that was forwarded. <br>\n",
    "Signature: The extracted signature from the body.<br>\n",
    "Date: Time the email was sent. <br>\n",
    "\n",
    "3. During the email processing the full body of the email will be cleaned using a combination of delimiter splitting, and the talon email cleaning library to extract signatures. \n",
    "\n",
    "4. Null values and column types will be set or cleaned up.\n",
    "\n",
    "5. The body of the email will be tokenized into seperate sentences then appended to the dataframe:\n",
    "\n",
    "Extractive_Sentences: Used to help generate the full summary. <br>\n",
    "Tokenized_Body: used as an input for the ML model. <br>\n",
    "\n",
    "6. The pandas dataframe will be saved for future notebooks. \n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirt/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import mailparser\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import email\n",
    "import numpy as np\n",
    "from talon.signature.bruteforce import extract_signature\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local filepaths\n",
    "mail_dir = '../data/maildir/'\n",
    "#Used to look at single mailbox. \n",
    "#mailbox = \"skilling-j\" \n",
    "#PICKLE_LOC = \"../data/dataframes/wrangled_enron_df.pkl\"\n",
    "#following location is for full dataframe\n",
    "ENRON_PICKLE_LOC = \"../data/dataframes/wrangled_enron_full_df.pkl\"\n",
    "BC3_EMAIL_PICKLE_LOC = \"../data/dataframes/wrangled_BC3_email_df.pkl\"\n",
    "BC3_SUMMARY_PICKLE_LOC = \"../data/dataframes/wrangled_BC3_summary_df.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the function that will clean up the email body. The following splits the chain emails from the actual body of the email. It appears that \"----Original Message-----\" indicates a forwarded part of an email so this is one of the delimiters. \n",
    "\n",
    "There are also emails that don't seem to necessarily be part of a chain, but would be referencing another email, which means more email metadata would be found in the body. If this meta data is found first, then the email will be split there instead. The goal is to cut out the largest amount of email forwarding and related metadata to extract only the written message by the original sender. \n",
    "\n",
    "Lastly Talon will be used to extract the signatures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_body(mail_body):\n",
    "    delimiters = [\"-----Original Message-----\",\"To:\",\"From\"]\n",
    "    \n",
    "    #Trying to split string by biggest delimiter. \n",
    "    old_len = sys.maxsize\n",
    "    \n",
    "    for delimiter in delimiters:\n",
    "        split_body = mail_body.split(delimiter,1)\n",
    "        new_len = len(split_body[0])\n",
    "        if new_len <= old_len:\n",
    "            old_len = new_len\n",
    "            final_split = split_body\n",
    "            \n",
    "    #Then pull chain message\n",
    "    if (len(final_split) == 1):\n",
    "        mail_chain = None\n",
    "    else:\n",
    "        mail_chain = final_split[1] \n",
    "    \n",
    "    #The following uses Talon to try to get a clean body, and seperate out the rest of the email. \n",
    "    clean_body, sig = extract_signature(final_split[0])\n",
    "    \n",
    "    return {'Body': clean_body, 'Chain' : mail_chain, 'Signature': sig}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function processes the email and appends the parsed pieces into a pandas dataframe. We need to start with a list, that will represent each row of the dataframe. Each item in the row is a dict of parsed email values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date(date_time):\n",
    "    try:\n",
    "        date_time = email.utils.format_datetime(email.utils.parsedate_to_datetime(date_time))\n",
    "    except:\n",
    "        date_time = None\n",
    "    return date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_email_list = []\n",
    "def process_email(email_path, employee, folder):\n",
    "    mail = mailparser.parse_from_file(email_path)\n",
    "    full_body = email.message_from_string(mail.body)\n",
    "    \n",
    "    #Only getting first payload\n",
    "    if full_body.is_multipart():\n",
    "        return\n",
    "    else:\n",
    "        mail_body = full_body.get_payload()    \n",
    "    \n",
    "    split_body = clean_body(mail_body)\n",
    "    headers = mail.headers\n",
    "    #Reformating date to be more pandas readable\n",
    "    date_time = process_date(headers.get('Date'))\n",
    "\n",
    "    email_dict = {\n",
    "                \"Employee\" : employee,\n",
    "                \"Email Folder\": folder,\n",
    "                \"Message-ID\": headers.get('Message-ID'),\n",
    "                \"Date\" : date_time,\n",
    "                \"From\" : headers.get('From'),\n",
    "                \"Subject\": headers.get('Subject'),\n",
    "                \"Body\" : split_body['Body'],\n",
    "                \"Chain\" : split_body['Chain'],\n",
    "                \"Signature\": split_body['Signature'],\n",
    "                \"Full_Email_Path\" : email_path #for debug purposes. \n",
    "    }\n",
    "    \n",
    "    #Append row to dataframe. \n",
    "    enron_email_list.append(email_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Email body cleaning at dataframe level\n",
    "def clean_email_df(df):\n",
    "    #Removing strings related to attatchments and certain non numerical characters.\n",
    "    patterns = [\"\\[IMAGE\\]\",\"-\", \"_\", \"\\*\"]\n",
    "    for pattern in patterns:\n",
    "        df['Body'] = pd.Series(df['Body']).str.replace(pattern, \"\")\n",
    "    \n",
    "    #Remove multiple spaces. \n",
    "    df.Body = df.Body.replace('\\s+', ' ', regex=True)\n",
    "\n",
    "    #Blanks are replaced with NaN in the whole dataframe. Then rows with a 'NaN' in the body will be dropped. \n",
    "    df = df.replace('',np.NaN)\n",
    "    df = df.dropna(subset=['Body'])\n",
    "\n",
    "    #Remove all Duplicate emails \n",
    "    df = df.drop_duplicates(subset='Body')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fully tokenize the Body of each email. \n",
    "#This function removes stopwords\n",
    "def remove_stopwords(sen):\n",
    "    stop_words = stopwords.words('english')\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new\n",
    "\n",
    "#This function splits up the body into sentence tokens and removes stop words. \n",
    "def tokenize_email(text):\n",
    "    clean_sentences = sent_tokenize(text, language='english')\n",
    "    #removing punctuation, numbers and special characters. Then lowercasing. \n",
    "    clean_sentences = [re.sub('[^a-zA-Z ]', '',s) for s in clean_sentences]\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go through each person's inbox then load up each email to be cleaned and added to the dataframe. \n",
    "mailboxes = listdir(mail_dir)\n",
    "for mailbox in mailboxes:\n",
    "    inbox = listdir(mail_dir + mailbox)\n",
    "    for folder in inbox:\n",
    "        path = mail_dir + mailbox + \"/\" + folder\n",
    "        emails = listdir(path)\n",
    "        for single_email in emails:\n",
    "            full_path = path + \"/\" + single_email\n",
    "            process_email(full_path, mailbox, folder)\n",
    "    enron_email_list_df = pd.DataFrame(enron_email_list)\n",
    "\n",
    "#Convert date to pandas datetime.\n",
    "enron_email_list_df['Date'] = pd.to_datetime(enron_email_list_df.Date)\n",
    "enron_email_list_df = clean_email_df(enron_email_list_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This tokenizing will be the extracted sentences that may be chosen to form the email summaries. \n",
    "enron_email_list_df['Extractive_Sentences'] = enron_email_list_df['Body'].apply(sent_tokenize)\n",
    "\n",
    "#Splitting the text in emails into cleaned sentences\n",
    "enron_email_list_df['Tokenized_Body'] = enron_email_list_df['Body'].apply(tokenize_email)\n",
    "\n",
    "#Tokenizing the bodies might have revealed more duplicate emails, or extremely similar emails that should be droped. \n",
    "enron_email_list_df = enron_email_list_df.loc[enron_email_list_df.astype(str).drop_duplicates(subset='Tokenized_Body').index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_email_list_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the cleaned DataFrame to disk. \n",
    "enron_email_list_df.to_pickle(ENRON_PICKLE_LOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2, BC3 Corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 of this notebook wrangles the BC3 Email Corpus which consists of 40 email threads (3222 sentences) from the W3C corpus. These set of emails contain human summaries of each email, which will be useful to evaluate the performance of the text summarization algorithim. \n",
    "\n",
    "This dataset is split into two xml files. One contains the original emails split line by line, and the other contains the summarizations created by the annotators. Each email may contain several summarizations from different annotators. Summarizations may also be over several emails. \n",
    "\n",
    "I will link the two data frames using the thread number in combination of the email number. \n",
    "\n",
    "1. The first dataframe will contain the wrangled original emails containing the following information\n",
    "\n",
    "Listno: Thread identifier\n",
    "Email_num: Email in thread sequence\n",
    "From: The original sender of the email\n",
    "To: The recipient of the email. \n",
    "Recieved: Time email was recieved. \n",
    "Subject: Title of email.\n",
    "Body: Original body.\n",
    "\n",
    "2. The second dataframe contains the summarizations of each email containing the following:\n",
    "\n",
    "Annotator: Person who created summarization. \n",
    "Email_num: Email in thread sequence.\n",
    "Listno: Thread identifier.\n",
    "Summary: Human summarization of the email. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bc3_emails(root):\n",
    "    BC3_email_list = []\n",
    "    #The emails are seperated by threads.\n",
    "    for thread in root:\n",
    "        email_num = 0\n",
    "        #Iterate through the thread elements <name, listno, Doc>\n",
    "        for thread_element in thread:\n",
    "            #Getting the listno allows us to link the summaries to the correct emails\n",
    "            if thread_element.tag == \"listno\":\n",
    "                listno = thread_element.text\n",
    "            #Each Doc element is a single email\n",
    "            if thread_element.tag == \"DOC\":\n",
    "                email_num += 1\n",
    "                email_metadata = []\n",
    "                for email_attribute in thread_element:\n",
    "                    #If the email_attri is text, then each child contains a line from the body of the email\n",
    "                    if email_attribute.tag == \"Text\":\n",
    "                        email_body = \"\"\n",
    "                        for sentence in email_attribute:\n",
    "                            email_body += sentence.text\n",
    "                    else:\n",
    "                        #The attributes of the Email <Recieved, From, To, Subject, Text> appends in this order. \n",
    "                        email_metadata.append(email_attribute.text)\n",
    "                        \n",
    "                #Use same enron cleaning methods on the body of the email\n",
    "                split_body = clean_body(email_body)\n",
    "                    \n",
    "                email_dict = {\n",
    "                    \"Listno\" : listno,\n",
    "                    \"Date\" : process_date(email_metadata[0]),\n",
    "                    \"From\" : email_metadata[1],\n",
    "                    \"To\" : email_metadata[2],\n",
    "                    \"Subject\" : email_metadata[3],\n",
    "                    \"Body\" : split_body['Body'],\n",
    "                    \"Email_num\": email_num\n",
    "                }\n",
    "                \n",
    "                BC3_email_list.append(email_dict)           \n",
    "    return pd.DataFrame(BC3_email_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bc3_summaries(root):\n",
    "    BC3_summary_list = []\n",
    "    for thread in root:\n",
    "        #Iterate through the thread elements <listno, name, annotation>\n",
    "        for thread_element in thread:\n",
    "            if thread_element.tag == \"listno\":\n",
    "                listno = thread_element.text\n",
    "            #Each Doc element is a single email\n",
    "            if thread_element.tag == \"annotation\":\n",
    "                for annotation in thread_element:\n",
    "                #If the email_attri is summary, then each child contains a summarization line\n",
    "                    if annotation.tag == \"summary\":\n",
    "                        summary_dict = {}\n",
    "                        for summary in annotation:\n",
    "                            #Generate the set of emails the summary sentence belongs to (often a single email)\n",
    "                            email_nums = summary.attrib['link'].split(',')\n",
    "                            s = set()\n",
    "                            for num in email_nums:\n",
    "                                s.add(num.split('.')[0].strip()) \n",
    "                            #Remove empty strings, since they summarize whole threads instead of emails. \n",
    "                            s = [x for x in set(s) if x]\n",
    "                            for email_num in s:\n",
    "                                if email_num in summary_dict:\n",
    "                                    summary_dict[email_num] += ' ' + summary.text\n",
    "                                else:\n",
    "                                    summary_dict[email_num] = summary.text\n",
    "                    #get annotator description\n",
    "                    elif annotation.tag == \"desc\":\n",
    "                        annotator = annotation.text\n",
    "                #For each email summarizaiton create an entry\n",
    "                for email_num, summary in summary_dict.items():\n",
    "                    email_dict = {\n",
    "                        \"Listno\" : listno,\n",
    "                        \"Annotator\" : annotator,\n",
    "                        \"Email_num\" : email_num,\n",
    "                        \"Summary\" : summary\n",
    "                    }      \n",
    "                    BC3_summary_list.append(email_dict)\n",
    "    return pd.DataFrame(BC3_summary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>Date</th>\n",
       "      <th>Email_num</th>\n",
       "      <th>From</th>\n",
       "      <th>Listno</th>\n",
       "      <th>Subject</th>\n",
       "      <th>To</th>\n",
       "      <th>Extractive_Sentences</th>\n",
       "      <th>Tokenized_Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The IETF meetings tend to become too large, creating logistics and planning problems. I suggest that future meetings are held for two weeks, with applications and user services issues the first week, and all other issues the second week. Those who so wish could attend both weeks, and other people could attend only one week. Those who choose to attend both weeks would be able to cover more groups and do better liaisons between the different areas. The Friday of the first week could discuss applications issues which might be of special interest to the other areas, and the Monday of the second week would schedule other groups which might be of special interest to applications people, so some people could attend MondayMonday or FridayFriday. Jacob Palme &amp;lt;jpalme@dsv.su.se&amp;gt; (Stockholm University and KTH) for more info see URL: http://www.dsv.su.se/~jpalme</td>\n",
       "      <td>1998-12-08 15:30:52+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Jacob Palme &lt;jpalme@dsv.su.se&gt;</td>\n",
       "      <td>007-7484738</td>\n",
       "      <td>Extending IETF meetings to two weeks?</td>\n",
       "      <td>discuss@apps.ietf.org</td>\n",
       "      <td>[The IETF meetings tend to become too large, creating logistics and planning problems., I suggest that future meetings are held for two weeks, with applications and user services issues the first week, and all other issues the second week., Those who so wish could attend both weeks, and other people could attend only one week., Those who choose to attend both weeks would be able to cover more groups and do better liaisons between the different areas., The Friday of the first week could discuss applications issues which might be of special interest to the other areas, and the Monday of the second week would schedule other groups which might be of special interest to applications people, so some people could attend MondayMonday or FridayFriday., Jacob Palme &amp;lt;jpalme@dsv.su.se&amp;gt; (Stockholm University and KTH) for more info see URL: http://www.dsv.su.se/~jpalme]</td>\n",
       "      <td>[ietf meetings tend become large creating logistics planning problems, suggest future meetings held two weeks applications user services issues first week issues second week, wish could attend weeks people could attend one week, choose attend weeks would able cover groups better liaisons different areas, friday first week could discuss applications issues might special interest areas monday second week would schedule groups might special interest applications people people could attend mondaymonday fridayfriday, jacob palme ltjpalmedsvsusegt stockholm university kth info see url httpwwwdsvsusejpalme]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Body  \\\n",
       "0  The IETF meetings tend to become too large, creating logistics and planning problems. I suggest that future meetings are held for two weeks, with applications and user services issues the first week, and all other issues the second week. Those who so wish could attend both weeks, and other people could attend only one week. Those who choose to attend both weeks would be able to cover more groups and do better liaisons between the different areas. The Friday of the first week could discuss applications issues which might be of special interest to the other areas, and the Monday of the second week would schedule other groups which might be of special interest to applications people, so some people could attend MondayMonday or FridayFriday. Jacob Palme &lt;jpalme@dsv.su.se&gt; (Stockholm University and KTH) for more info see URL: http://www.dsv.su.se/~jpalme   \n",
       "\n",
       "                       Date  Email_num                            From  \\\n",
       "0 1998-12-08 15:30:52+00:00  1          Jacob Palme <jpalme@dsv.su.se>   \n",
       "\n",
       "        Listno                                Subject                     To  \\\n",
       "0  007-7484738  Extending IETF meetings to two weeks?  discuss@apps.ietf.org   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Extractive_Sentences  \\\n",
       "0  [The IETF meetings tend to become too large, creating logistics and planning problems., I suggest that future meetings are held for two weeks, with applications and user services issues the first week, and all other issues the second week., Those who so wish could attend both weeks, and other people could attend only one week., Those who choose to attend both weeks would be able to cover more groups and do better liaisons between the different areas., The Friday of the first week could discuss applications issues which might be of special interest to the other areas, and the Monday of the second week would schedule other groups which might be of special interest to applications people, so some people could attend MondayMonday or FridayFriday., Jacob Palme &lt;jpalme@dsv.su.se&gt; (Stockholm University and KTH) for more info see URL: http://www.dsv.su.se/~jpalme]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Tokenized_Body  \n",
       "0  [ietf meetings tend become large creating logistics planning problems, suggest future meetings held two weeks applications user services issues first week issues second week, wish could attend weeks people could attend one week, choose attend weeks would able cover groups better liaisons different areas, friday first week could discuss applications issues might special interest areas monday second week would schedule groups might special interest applications people people could attend mondaymonday fridayfriday, jacob palme ltjpalmedsvsusegt stockholm university kth info see url httpwwwdsvsusejpalme]  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load BC3 Email Corpus \n",
    "parsedXML = ET.parse( \"../data/BC3_Email_Corpus/corpus.xml\" )\n",
    "root = parsedXML.getroot()\n",
    "\n",
    "#Clean up BC3 email body's same way as Enron emails\n",
    "bc3_email_df = parse_bc3_emails(root)\n",
    "bc3_email_df = clean_email_df(bc3_email_df)\n",
    "bc3_email_df['Date'] = pd.to_datetime(bc3_email_df.Date, utc=True)\n",
    "\n",
    "#Tokenize emails \n",
    "bc3_email_df['Extractive_Sentences'] = bc3_email_df['Body'].apply(sent_tokenize)\n",
    "bc3_email_df['Tokenized_Body'] = bc3_email_df['Body'].apply(tokenize_email)\n",
    "\n",
    "#Tokenizing the bodies might have revealed more duplicate emails, or extremely similar emails that should be droped. \n",
    "bc3_email_df = bc3_email_df.loc[bc3_email_df.astype(str).drop_duplicates(subset='Tokenized_Body').index]\n",
    "bc3_email_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load summaries and process\n",
    "parsedXML = ET.parse( \"../data/BC3_Email_Corpus/annotation.xml\" )\n",
    "root = parsedXML.getroot()\n",
    "bc3_summary_df = parse_bc3_summaries(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store dataframes to disk\n",
    "bc3_email_df.to_pickle(BC3_EMAIL_PICKLE_LOC)\n",
    "bc3_summary_df.to_pickle(BC3_SUMMARY_PICKLE_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annotator</th>\n",
       "      <th>Email_num</th>\n",
       "      <th>Listno</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Annotator7-Part2</td>\n",
       "      <td>1</td>\n",
       "      <td>067-11978590</td>\n",
       "      <td>Wendy states that they are moving forward with a face to face meeting in Bristol, England on Oct 5-6. The Authoring tool group will join them to discuss revisions to the WCAG on the 6th. She asks the group if they like to discuss the draft they have now and asks if they want to work on a draft for 2 full days.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Annotator7-Part2</td>\n",
       "      <td>2</td>\n",
       "      <td>067-11978590</td>\n",
       "      <td>William responds that it is mandatory to discuss the draft to avoid conflicts and oversights. He thinks it'll take greater than 2 days to work on the draft. He is looking forward to the meeting in Bristol.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annotator7-Part2</td>\n",
       "      <td>3</td>\n",
       "      <td>067-11978590</td>\n",
       "      <td>Jonathan asks he is attending the venue, what the agenda is and if it would be useful if he attended with a presentation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Annotator7-Part2</td>\n",
       "      <td>4</td>\n",
       "      <td>067-11978590</td>\n",
       "      <td>Charles responds and says it suits him to meet and discuss the draft.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Annotator7-Part2</td>\n",
       "      <td>5</td>\n",
       "      <td>067-11978590</td>\n",
       "      <td>Wendy answers all of Jonathan's questions: registration has not started, there is no agenda yet, and yes it would be worthwhile if Jonathan attended with a presentation.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Annotator Email_num        Listno  \\\n",
       "0  Annotator7-Part2  1         067-11978590   \n",
       "1  Annotator7-Part2  2         067-11978590   \n",
       "2  Annotator7-Part2  3         067-11978590   \n",
       "3  Annotator7-Part2  4         067-11978590   \n",
       "4  Annotator7-Part2  5         067-11978590   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                   Summary  \n",
       "0  Wendy states that they are moving forward with a face to face meeting in Bristol, England on Oct 5-6. The Authoring tool group will join them to discuss revisions to the WCAG on the 6th. She asks the group if they like to discuss the draft they have now and asks if they want to work on a draft for 2 full days.  \n",
       "1  William responds that it is mandatory to discuss the draft to avoid conflicts and oversights. He thinks it'll take greater than 2 days to work on the draft. He is looking forward to the meeting in Bristol.                                                                                                            \n",
       "2  Jonathan asks he is attending the venue, what the agenda is and if it would be useful if he attended with a presentation.                                                                                                                                                                                                \n",
       "3  Charles responds and says it suits him to meet and discuss the draft.                                                                                                                                                                                                                                                    \n",
       "4  Wendy answers all of Jonathan's questions: registration has not started, there is no agenda yet, and yes it would be worthwhile if Jonathan attended with a presentation.                                                                                                                                                "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc3_summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'> The IETF meetings tend to become too large, creating logistics and planning problems. ... My problem over the past year or so is that there are only a few session I wish to attend, but I cannot know for sure when they will be scheduled, so I cannot make reasonable travel arrangements (a week in Orlando for 6 hours of meetings is hard to sell to management). Now I know there is a rationale here, and that one is encouraged to participate broadly. And I am hopeful that new activities (my own and in the IETF) will give me many more reasons to attend. But firmer scheduling would be a big win. regards, Terry Terry Allen Electronic Commerce and Publishing Consultant tallen[at]sonic.net http://www.sonic.net/~tallen/ DocBook: http://www.ora.com/davenport/index.html Common Business Library: http://www.veosystems.com/'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at single thread email. \n",
    "bc3_email_df.iloc[1]['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118    Terry supported Jacob's idea and suggested a firmer scheduling.             \n",
       "124    Some argue that it'd be more useful to prepare the meeting schedule earlier.\n",
       "Name: Summary, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at summaries \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "bc3_summary_df['Summary'].loc[(bc3_summary_df['Listno'] == bc3_email_df.iloc[0]['Listno']) & (bc3_summary_df['Email_num'] == '2')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
