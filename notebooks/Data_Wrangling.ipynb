{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to clean the Enron Email data set to perform email text summarization: Given a single person’s folder and a time range give a summary for each email. \n",
    "\n",
    "My approach to cleaning the data involves the following steps:\n",
    "\n",
    "1. I have unpacked the maildir directory from the Enron data set. I will be focusing on each person’s inbox, which means all other folders will be ignored for now.\n",
    "\n",
    "2. I will process each inbox email to add the following information to a Pandas data frame:\n",
    "\n",
    "Employee: The username of the email owner. <br>\n",
    "Body: Cleaned body of the email. <br>\n",
    "Subject: The title of the email. <br>\n",
    "From: The original sender of the email <br>\n",
    "Message-ID: Used to remove duplicate emails, as each email has a unique ID. <br>\n",
    "Chain: The parsed out email chain from a email that was forwarded. <br>\n",
    "Signature: The extracted signature from the body.<br>\n",
    "Date: Time the email was sent. <br>\n",
    "\n",
    "3. During the email processing the full body of the email will be cleaned using a combination of delimiter splitting, and the talon email cleaning library to extract signatures. \n",
    "\n",
    "4. Null values and column types will be set or cleaned up.\n",
    "\n",
    "5. The body of the email will be tokenized into seperate sentences. Extractive sentences will be used to help generate the full summary, the tokenized body is used for the ML model. \n",
    "\n",
    "6. The pandas dataframe will be saved for future notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mailparser\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import email\n",
    "import numpy as np\n",
    "from talon.signature.bruteforce import extract_signature\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local filepaths\n",
    "mail_dir = '../data/maildir/'\n",
    "mailbox = \"skilling-j\" #Start with single mailbox. \n",
    "PICKLE_LOC = \"../data/dataframes/wrangled_enron_df.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the function that will clean up the email body. The following splits the chain emails from the actual body of the email. It appears that \"----Original Message-----\" indicates a forwarded part of an email so this is one of the delimiters. \n",
    "\n",
    "There are also emails that don't seem to necessarily be part of a chain, but would be referencing another email, which means more email metadata would be found in the body. If this meta data is found first, then the email will be split there instead. The goal is to cut out the largest amount of email forwarding and related metadata to extract only the written message by the original sender. \n",
    "\n",
    "Lastly Talon will be used to extract the signatures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_body(mail_body):\n",
    "    delimiters = [\"-----Original Message-----\",\"To:\",\"From\"]\n",
    "    \n",
    "    #Trying to split string by biggest delimiter. \n",
    "    old_len = sys.maxsize\n",
    "    \n",
    "    for delimiter in delimiters:\n",
    "        split_body = mail_body.split(delimiter,1)\n",
    "        new_len = len(split_body[0])\n",
    "        if new_len <= old_len:\n",
    "            old_len = new_len\n",
    "            final_split = split_body\n",
    "            \n",
    "    #Then pull chain message\n",
    "    if (len(final_split) == 1):\n",
    "        mail_chain = None\n",
    "    else:\n",
    "        mail_chain = final_split[1] \n",
    "    \n",
    "    #The following uses Talon to try to get a clean body, and seperate out the rest of the email. \n",
    "    clean_body, sig = extract_signature(final_split[0])\n",
    "    \n",
    "    return {'Body': clean_body, 'Chain' : mail_chain, 'Signature': sig}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function processes the email and appends the parsed pieces into a pandas dataframe. We need to start with a list, that will represent each row of the dataframe. Each item in the row is a dict of parsed email values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_email_list = []\n",
    "def process_email(email_path, employee, folder, email_name):\n",
    "    mail = mailparser.parse_from_file(email_path)\n",
    "    full_body = email.message_from_string(mail.body)\n",
    "    \n",
    "    #Only getting first payload\n",
    "    if full_body.is_multipart():\n",
    "        return\n",
    "    else:\n",
    "        mail_body = full_body.get_payload()    \n",
    "    \n",
    "    split_body = clean_body(mail_body)\n",
    "    headers = mail.headers\n",
    "    #Reformating date to be more pandas readable\n",
    "    date_time = headers.get('Date')\n",
    "    \n",
    "    if date_time:\n",
    "        date_time = email.utils.format_datetime(email.utils.parsedate_to_datetime(date_time))\n",
    "    \n",
    "    email_dict = {\n",
    "                \"Employee\" : employee,\n",
    "                \"Email Folder\": folder,\n",
    "                \"Message-ID\": headers.get('Message-ID'),\n",
    "                \"Date\" : date_time,\n",
    "                \"From\" : headers.get('From'),\n",
    "                \"Subject\": headers.get('Subject'),\n",
    "                \"Body\" : split_body['Body'],\n",
    "                \"Chain\" : split_body['Chain'],\n",
    "                \"Signature\": split_body['Signature'],\n",
    "                \"Full_Email_Path\" : email_path #for debug purposes. \n",
    "    }\n",
    "    \n",
    "    #Append row to dataframe. \n",
    "    enron_email_list.append(email_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go through each person's inbox then load up each email to be cleaned and added to the dataframe. \n",
    "\n",
    "#mailboxes = listdir(mail_dir)\n",
    "#for mailbox in mailboxes:\n",
    "inbox = listdir(mail_dir + mailbox)\n",
    "for folder in inbox:\n",
    "    path = mail_dir + mailbox + \"/\" + folder\n",
    "    emails = listdir(path)\n",
    "    for single_email in emails:\n",
    "        full_path = path + \"/\" + single_email\n",
    "        process_email(full_path, mailbox, folder, single_email)\n",
    "enron_email_list_df = pd.DataFrame(enron_email_list)\n",
    "#enron_email_list_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert date to pandas datetime.\n",
    "enron_email_list_df['Date'] = pd.to_datetime(enron_email_list_df.Date)\n",
    "\n",
    "#Blanks are replaced with NaN in the whole dataframe. Then rows with a 'NaN' in the body will be dropped. \n",
    "enron_email_list_df = enron_email_list_df.replace('',np.NaN)\n",
    "enron_email_list_df = enron_email_list_df.dropna(subset=['Body'])\n",
    "\n",
    "#Remove all Duplicate emails \n",
    "enron_email_list_df = enron_email_list_df.drop_duplicates(subset='Body')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing strings related to attatchments and certain non numerical characters. This is to improve the extractive summary output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\"\\[IMAGE\\]\",\"-\", \"_\", \"\\*\"]\n",
    "for pattern in patterns:\n",
    "    enron_email_list_df['Body'] = pd.Series(enron_email_list_df['Body']).str.replace(pattern, \"\")\n",
    "    \n",
    "#Remove multiple spaces. \n",
    "enron_email_list_df.Body = enron_email_list_df.Body.replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fully tokenize the Body of each email. \n",
    "\n",
    "#This function removes stopwords\n",
    "def remove_stopwords(sen):\n",
    "    stop_words = stopwords.words('english')\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new\n",
    "\n",
    "#This function splits up the body into sentence tokens and removes stop words. \n",
    "def tokenize_email(text):\n",
    "    clean_sentences = sent_tokenize(text, language='english')\n",
    "    #removing punctuation, numbers and special characters. Then lowercasing. \n",
    "    clean_sentences = [re.sub('[^a-zA-Z ]', '',s) for s in clean_sentences]\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "    return clean_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This tokenizing will be the extracted sentences that may be chosen to form the email summaries. \n",
    "enron_email_list_df['Extractive_Sentences'] = enron_email_list_df['Body'].apply(sent_tokenize)\n",
    "\n",
    "#Splitting the text in emails into cleaned sentences\n",
    "enron_email_list_df['Tokenized_Body'] = enron_email_list_df['Body'].apply(tokenize_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the cleaned DataFrame to disk. \n",
    "enron_email_list_df.to_pickle(PICKLE_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
